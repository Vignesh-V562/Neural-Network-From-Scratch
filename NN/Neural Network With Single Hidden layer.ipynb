{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8a8c32a0-27f9-48d1-9823-4a47d2c80e10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "328f54a0-c151-49a3-8e81-1b5e0b53e1b2",
   "metadata": {},
   "source": [
    "## Neural Network model\n",
    "<img src=\"Images/classification_kiank.png\" style=\"width:600px;height:300px;\">\n",
    "\n",
    "**Mathematically**:\n",
    "\n",
    "For one example $x^{(i)}$:\n",
    "$$z^{[1] (i)} =  W^{[1]} x^{(i)} + b^{[1]}$$ \n",
    "$$a^{[1] (i)} = \\tanh(z^{[1] (i)})$$\n",
    "$$z^{[2] (i)} = W^{[2]} a^{[1] (i)} + b^{[2]}$$\n",
    "$$\\hat{y}^{(i)} = a^{[2] (i)} = \\sigma(z^{ [2] (i)})$$\n",
    "$$y^{(i)}_{prediction} = \\begin{cases} 1 & \\mbox{if } a^{[2](i)} > 0.5 \\\\ 0 & \\mbox{otherwise } \\end{cases}$$\n",
    "\n",
    "Given the predictions on all the examples, you can also compute the cost $J$ as follows: \n",
    "$$J = - \\frac{1}{m} \\sum\\limits_{i = 0}^{m} \\large\\left(\\small y^{(i)}\\log\\left(a^{[2] (i)}\\right) + (1-y^{(i)})\\log\\left(1- a^{[2] (i)}\\right)  \\large  \\right) \\small $$\n",
    "\n",
    "**Reminder**: The general methodology to build a Neural Network is to:\n",
    "1. Define the neural network structure ( # of input units,  # of hidden units, etc). \n",
    "2. Initialize the model's parameters\n",
    "3. Loop:\n",
    "   - Implement forward propagation\n",
    "    - Compute loss\n",
    "    - Implement backward propagation to get the gradients\n",
    "    - Update parameters (gradient descent)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfe10207-6fce-4dc3-b20c-a8414842392b",
   "metadata": {},
   "source": [
    "### layer_sizes \n",
    "Define three variables:\n",
    "- n_x: the size of the input layer\n",
    "- n_h: the size of the hidden layer  \n",
    "- n_y: the size of the output layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ba06934a-da25-4e54-95c7-f02d36325702",
   "metadata": {},
   "outputs": [],
   "source": [
    "def layer_sizes(X, Y):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    X -- input dataset of shape (input size, number of examples)\n",
    "    Y -- labels of shape (output size, number of examples)\n",
    "    \"\"\"\n",
    "    \n",
    "    n_x = X.shape[0]\n",
    "    n_h = 4 # Optional\n",
    "    n_y = Y.shape[0]\n",
    "    \n",
    "    return (n_x, n_h, n_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8bc68239-0889-42af-a427-71c9ef51f074",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    z -- A scalar or numpy array of any size.\n",
    "\n",
    "    Return:\n",
    "    s -- sigmoid(z)\n",
    "    \"\"\"\n",
    "    s = 1 / (1 + np.exp(-z))\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f8408556-8d4a-4d70-9605-05f7331a0d75",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters(n_x, n_h, n_y):\n",
    "    \"\"\"\n",
    "    Argument:\n",
    "    n_x -- size of the input layer\n",
    "    n_h -- size of the hidden layer\n",
    "    n_y -- size of the output layer\n",
    "    \"\"\"    \n",
    "    W1 = np.random.randn(n_h,n_x) * 0.01\n",
    "    b1 = np.zeros((n_h,1))\n",
    "    W2 = np.random.randn(n_y,n_h) * 0.01\n",
    "    b2 = np.zeros((n_y,1))\n",
    "\n",
    "    parameters = {\"W1\": W1,\n",
    "                  \"b1\": b1,\n",
    "                  \"W2\": W2,\n",
    "                  \"b2\": b2}\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e923650e-6a82-4cbd-b7c3-ea8bfe726b5e",
   "metadata": {},
   "source": [
    "### forward_propagation\n",
    "\n",
    "Implement `forward_propagation()` using the following equations:\n",
    "\n",
    "$$Z^{[1]} =  W^{[1]} X + b^{[1]}\\tag{1}$$ \n",
    "$$A^{[1]} = \\tanh(Z^{[1]})\\tag{2}$$\n",
    "$$Z^{[2]} = W^{[2]} A^{[1]} + b^{[2]}\\tag{3}$$\n",
    "$$\\hat{Y} = A^{[2]} = \\sigma(Z^{[2]})\\tag{4}$$\n",
    "\n",
    "- Implement using these steps:\n",
    "    1. Retrieve each parameter from the dictionary \"parameters\" (which is the output of `initialize_parameters()` by using `parameters[\"..\"]`.\n",
    "    2. Implement Forward Propagation. Compute $Z^{[1]}, A^{[1]}, Z^{[2]}$ and $A^{[2]}$ (the vector of all your predictions on all the examples in the training set).\n",
    "- Values needed in the backpropagation are stored in \"cache\". The cache will be given as an input to the backpropagation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a1d77b3e-32ee-4295-9539-a12cc8399a07",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagation(X, parameters):\n",
    "    \"\"\"\n",
    "    Argument:\n",
    "    X -- input data of size \n",
    "    parameters -- python dictionary containing your parameters \n",
    "    \"\"\"\n",
    "    \n",
    "    W1 = parameters[\"W1\"]\n",
    "    b1 = parameters[\"b1\"]\n",
    "    W2 = parameters[\"W2\"]\n",
    "    b2 = parameters[\"b2\"]\n",
    "    \n",
    "    Z1 = np.dot(W1,X) + b1\n",
    "    A1 = np.tanh(Z1)\n",
    "    Z2 = np.dot(W2,A1) + b2\n",
    "    A2 = sigmoid(Z2)\n",
    "    \n",
    "    cache = {\"Z1\": Z1,\n",
    "             \"A1\": A1,\n",
    "             \"Z2\": Z2,\n",
    "             \"A2\": A2}\n",
    "    \n",
    "    return A2, cache"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8623b52-96c6-4cbc-9007-b16de907831a",
   "metadata": {},
   "source": [
    "### Compute the Cost\n",
    "\n",
    "Now that we've computed $A^{[2]}$ (in the Python variable \"`A2`\"), which contains $a^{[2](i)}$ for all examples, you can compute the cost function as follows:\n",
    "\n",
    "$$J = - \\frac{1}{m} \\sum\\limits_{i = 1}^{m} \\large{(} \\small y^{(i)}\\log\\left(a^{[2] (i)}\\right) + (1-y^{(i)})\\log\\left(1- a^{[2] (i)}\\right) \\large{)} \\small\\$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1270864b-d863-4f5e-89d2-75ab8c601e79",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(A2, Y):\n",
    "    \"\"\"\n",
    "    Computes the cross-entropy cost given in equation \n",
    "    \n",
    "    Arguments:\n",
    "    A2 -- The sigmoid output of the second activation, of shape (1, number of examples)\n",
    "    Y -- \"true\" labels vector of shape (1, number of examples)\n",
    "    \"\"\"\n",
    "    \n",
    "    m = Y.shape[1] \n",
    "    cost = - (1/m) * np.sum(Y*np.log(A2) + (1-Y) * np.log(1 - A2))\n",
    "    \n",
    "    cost = float(np.squeeze(cost))  \n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e175456-8ee2-4108-a507-49fba96f94ec",
   "metadata": {},
   "source": [
    "### Implement Backpropagation\n",
    "\n",
    "Using the cache computed during forward propagation, now implement backward propagation.\n",
    "\n",
    "<img src=\"images/grad_summary.png\" style=\"width:600px;height:300px;\">\n",
    "<caption><center><font color='purple'><b>Figure 1</b>: Backpropagation. Use the six equations on the right.</font></center></caption>\n",
    "\n",
    "\n",
    "$\\frac{\\partial \\mathcal{J} }{ \\partial z_{2}^{(i)} } = \\frac{1}{m} (a^{[2](i)} - y^{(i)})$\n",
    "\n",
    "$\\frac{\\partial \\mathcal{J} }{ \\partial W_2 } = \\frac{\\partial \\mathcal{J} }{ \\partial z_{2}^{(i)} } a^{[1] (i) T} $\n",
    "\n",
    "$\\frac{\\partial \\mathcal{J} }{ \\partial b_2 } = \\sum_i{\\frac{\\partial \\mathcal{J} }{ \\partial z_{2}^{(i)}}}$\n",
    "\n",
    "$\\frac{\\partial \\mathcal{J} }{ \\partial z_{1}^{(i)} } =  W_2^T \\frac{\\partial \\mathcal{J} }{ \\partial z_{2}^{(i)} } * ( 1 - a^{[1] (i) 2}) $\n",
    "\n",
    "$\\frac{\\partial \\mathcal{J} }{ \\partial W_1 } = \\frac{\\partial \\mathcal{J} }{ \\partial z_{1}^{(i)} }  X^T $\n",
    "\n",
    "$\\frac{\\partial \\mathcal{J} _i }{ \\partial b_1 } = \\sum_i{\\frac{\\partial \\mathcal{J} }{ \\partial z_{1}^{(i)}}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1d4029c0-0fa3-4205-91f0-a0cc05eeb571",
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_propagation(parameters, cache, X, Y):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    parameters -- python dictionary containing our parameters \n",
    "    cache -- a dictionary containing \"Z1\", \"A1\", \"Z2\" and \"A2\".\n",
    "    X -- input data of shape (2, number of examples)\n",
    "    Y -- \"true\" labels vector of shape (1, number of examples)\n",
    "    \"\"\"\n",
    "    m = X.shape[1]\n",
    "    \n",
    "    W1 = parameters[\"W1\"]\n",
    "    W2 = parameters[\"W2\"]\n",
    "    \n",
    "    A1 = cache[\"A1\"]\n",
    "    A2 = cache[\"A2\"]\n",
    "    \n",
    "    dZ2 = A2 - Y\n",
    "    dW2 = (1/m) * np.dot(dZ2,A1.T)\n",
    "    db2 = (1/m) * np.sum(dZ2,axis = 1 ,keepdims = True)\n",
    "    dZ1 = np.dot(W2.T, dZ2) * (1 - np.power(A1,2))\n",
    "    dW1 = (1/m) * np.dot(dZ1,X.T)\n",
    "    db1 = (1/m) * np.sum(dZ1,axis = 1 ,keepdims = True)\n",
    "    \n",
    "    grads = {\"dW1\": dW1,\n",
    "             \"db1\": db1,\n",
    "             \"dW2\": dW2,\n",
    "             \"db2\": db2}\n",
    "    \n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fefda4d5-c322-4054-95ec-21eae7013bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters(parameters, grads, learning_rate = 1.2):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    parameters -- python dictionary containing your parameters \n",
    "    grads -- python dictionary containing your gradients \n",
    "    \"\"\"\n",
    "    W1 = copy.deepcopy(parameters[\"W1\"])\n",
    "    b1 = parameters[\"b1\"]\n",
    "    W2 = copy.deepcopy(parameters[\"W2\"])\n",
    "    b2 = parameters[\"b2\"]\n",
    "    \n",
    "    dW1 = grads[\"dW1\"]\n",
    "    db1 = grads[\"db1\"]\n",
    "    dW2 = grads[\"dW2\"]\n",
    "    db2 = grads[\"db2\"]\n",
    "    \n",
    "    W1 -= learning_rate* dW1\n",
    "    b1 -= learning_rate* db1\n",
    "    W2 -= learning_rate* dW2\n",
    "    b2 -= learning_rate* db2\n",
    "    \n",
    "    parameters = {\"W1\": W1,\n",
    "                  \"b1\": b1,\n",
    "                  \"W2\": W2,\n",
    "                  \"b2\": b2}\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "72f1ff8d-4dc9-4dd7-bb54-72882b9706c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nn_model(X, Y, n_h, num_iterations = 10000, print_cost=False):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    X -- dataset of shape (2, number of examples)\n",
    "    Y -- labels of shape (1, number of examples)\n",
    "    n_h -- size of the hidden layer\n",
    "    num_iterations -- Number of iterations in gradient descent loop\n",
    "    print_cost -- if True, print the cost every 1000 iterations\n",
    "    \"\"\"\n",
    "    \n",
    "    np.random.seed(3)\n",
    "    n_x = layer_sizes(X, Y)[0]\n",
    "    n_y = layer_sizes(X, Y)[2]\n",
    "    \n",
    "    parameters = initialize_parameters(n_x, n_h, n_y)\n",
    "\n",
    "    for i in range(0, num_iterations):\n",
    "\n",
    "        A2, cache =  forward_propagation(X, parameters)\n",
    "        cost = compute_cost(A2,Y)\n",
    "        grads = backward_propagation(parameters, cache, X, Y)\n",
    "        parameters = update_parameters(parameters, grads, learning_rate = 1.2)\n",
    "\n",
    "        if print_cost and i % 1000 == 0:\n",
    "            print (\"Cost after iteration %i: %f\" %(i, cost))\n",
    "\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "811d0e4c-a8ee-431d-85d8-035500323288",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(parameters, X):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    parameters -- python dictionary containing your parameters \n",
    "    X -- input data of size (n_x, m)\n",
    "    \"\"\"\n",
    "    A2 ,cache = forward_propagation(X, parameters)\n",
    "    predictions = (A2 > 0.5)\n",
    "    \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4c72399-5a59-4b68-8815-aba7f822379c",
   "metadata": {},
   "source": [
    "# Testing the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7ef8a08a-83ea-4676-8113-7f1ae7f330ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(569, 30)\n",
      "(569,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_breast_cancer \n",
    "from sklearn.model_selection import train_test_split\n",
    "dataset = load_breast_cancer()\n",
    "X = dataset.data\n",
    "y = dataset.target\n",
    "\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8e68974e-ba7e-4a6a-90d0-87d16ff158c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "X_train = X_train.T\n",
    "X_test = X_test.T\n",
    "y_train = y_train.reshape(1, -1)\n",
    "y_test = y_test.reshape(1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e8a357ab-4cae-4e52-8328-61e42b4b2910",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 0: 0.694911\n",
      "Cost after iteration 1000: 0.661242\n",
      "Cost after iteration 2000: 0.661242\n",
      "Cost after iteration 3000: 0.661242\n",
      "Cost after iteration 4000: 0.661242\n",
      "Cost after iteration 5000: 0.661242\n",
      "Cost after iteration 6000: 0.661242\n",
      "Cost after iteration 7000: 0.661242\n",
      "Cost after iteration 8000: 0.661242\n",
      "Cost after iteration 9000: 0.661242\n"
     ]
    }
   ],
   "source": [
    "parameters = nn_model(X_train,y_train,4,num_iterations = 10000,print_cost = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "efa72d36-1140-42fc-9a3f-d6dc6dbe834b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train Accuracy: 62.56%\n",
      "Test Accuracy: 63.16%\n"
     ]
    }
   ],
   "source": [
    "train_preds = predict(parameters, X_train)\n",
    "test_preds = predict(parameters, X_test)\n",
    "\n",
    "train_accuracy = 100 * np.mean(train_preds == y_train)\n",
    "test_accuracy = 100 * np.mean(test_preds == y_test)\n",
    "\n",
    "print(f\"\\nTrain Accuracy: {train_accuracy:.2f}%\")\n",
    "print(f\"Test Accuracy: {test_accuracy:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
